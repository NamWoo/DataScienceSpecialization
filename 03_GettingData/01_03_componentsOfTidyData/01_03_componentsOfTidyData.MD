0:00
In the last lecture, we talked about raw data, where you're starting from, the messy file that you're trying to extract some data from. In this lecture, we're going to be talking about tidy data. This is sort of the target, where you're going to take the raw data and turn it into a tidy data set, which you can then use to do downstream analysis that you might learn about in a regression modeling class or in the prediction end machine learning class.
0:21
So the four things that you should have when you finish going from a raw data set, to a tidy data set, are the following. So you should have the raw data, obviously that's the files that you actually extracting information from. You should have a tidy data set, and we're going to be talking about in a minute, and then you should have a code book describing each variable, and it's value in the tidy data set. This code book is often called the meta data. So it's the data that surrounds the data, so-to-speak, and explains what the data is trying to say. You might manage each column in your tidy data set corresponds to one variable, and you might want to describe things like the units of those variables in the code book.
0:58
And the critical part that sometimes is missing, but we're going to learn a lot about in this class, is an explicit and exact recipe that you use to go from steps one to steps two and three. In other words, you need to report the exact steps that you did, took to get from the raw data to the processed data. You can do that in a variety of different ways, but for the purposes of this class, we'll be putting together our scripts that then you could use to process data. And those will be the sort of recipe that you can hand off and say this is how I got from the raw to the tidy data.
1:28
So the raw data it's important to remember that there are different levels of raw data, as we talked about it the previous lecture, but for the purposes of your looking a particular data set the raw data are the rawest form of the data that you had access to. So for example, it might be a strange binary file that whatever your measuring spits out. It could be an unformatted Excel file with ten worksheets that some company you contracted with sent you, or it could be something that was handed to you as an Excel file, or it could be complicated JSON data you get from scraping an API. Or, it can even be something like handed your numbers you got from looking through a microscope and counting cells that appeared in that, in the window.
2:11
You know the raw data is in the right format if, and this is a critical one, you ran no software on the data. So, someone else might of run some software on the data, but by the time it handed to you, there was absolutely no software that was run. And, you didn't manipulate any of your numbers in the data set or move any of the data from the data set. And you didn't do any summarization to the data in any way. So this is how you know that the data is in its rawest form. And it's one component of the data that you should have, sort of the unadulterated, raw data, should be one component of, the having a tidy data set handed over to a collaborator.
2:45
The tidy data is, on the other hand, the target or the end goal of the whole process, and so the idea is you, you should have each variable that you've measured be in exactly one column. So there should be one variable per column, and each different observation should be in a different row. In other words, if you've measured a particular variable on a large number of people who have been tweeting, say you measure the number of tweets that were posted by a large number of users, then, what you would get is the number of tweets in the column and then, for every single user, you would get the number of tweets in that, in a different row. There should be one table for every kind of variable. So, for example, if you collect data from Twitter and Facebook, and so forth, you might have one table for each of those. If you have multiple tables you should definitely make sure, if you're trying to match them up, that they include a column in the table that allows them to be linked together. So this is often an id, and we'll talk a little bit about merging data sets later in the class.
3:45
A couple of other important tips that I've found have saved me a lot of trouble in the past is, if you include a row at the top of each file of variable names, sometimes this won't always appear in a tidy data-set, but it's much more useful if they are. It's also much more useful if the variable names are human readable. So for example, if you use something like AgeAtDiagnosis for a column name as opposed to AgeDx, which might be a little bit more confusing and harder for people to read. So its better be a little error on the side of being more explicit, and then in genera,l data should be saved in one file per table. So its a habit of some people to save data in multiple Excel spread sheets within a single Excel file, and so its a better idea to save each spread sheet in a different file. So we'll talk about that in the class. The next component is often a piece that is missing, which is the codebook. So you have this tidy data set that's very nice and clean and neat and you got that data set by doing a whole bunch of things to a broad data set. And so at the end of the day you end up with a data set that constitutes information about a bunch of different variables, and you might end up with just the variable names at the top of that file telling you what happened. But you might want to have more information about the variables. So one common example is, you might want to know the units. So the column header might be the amount of money that we made this quarter, and you'd really want to know if that was in units or thousands or millions or so forth.
5:16
You might also want to know some information about the summary choices that was made. So, it might be the variable measures something about the monthly revenue. And so you want to know whether the median or mean revenue was taken.
5:29
And then information about the experimental study design that you used. So you might want to know something about the way that you collected these data, whether it was just in a database, you extracted it out, or whether you performed an experiment, a randomized trial, or an A-B test or something like that.
5:46
So, a common format for this document is a Word or a text file. It'd be like a Markdown file. As you've seen, Markdown files are sort of a common used format in data science. There should definitely be a section called Study Design, and that has a thorough description of how you collected the data. So this should say things like how you picked which observations to collect. What did you extract out of the database, what did you exclude, and so forth.
6:11
There also should be a section called Code Book that describes all of the variables and it's units.
6:16
Finally, you need the instruction list. So, even if you collected all that information and you made it available in the certain terms of the tidy data, you should be able to go back to the raw data and re process it and get the same tidy data set. If that doesn't happen, then there's something going wrong in your data processing pipeline, and so, you want to be able to identify that and fix it. So ideally, this is going to be a computer script that will do this for you. And so, for the purposes of this class, of course R, but I guess, you know, if you have to, you can do it in Python, as well. The input for the script is the, raw data. And so, the output is going to be the processed, tidy data. And really good important component of this script is that there are no parameters. In other words, you fix everything that you've done, after you've done all the processing, and you have an exact recipe that doesn't have to be twe, tweaked or modified by the end user. And they will get the same tidy data set out if they put the same raw data set in.
7:12
So in some cases, it's not possible to script every step. So, for example, not everything you can do to data, you can do in R. I know it's hard to believe that I'm saying that, but it's true. And so, what you might end up having to do is have, in addition to the scri-, R scripts and the other scripts from the other programming languages you might use, you might need a set of commands that look like this where it's actually just written out in a text file. It says take the raw file, and you're going to run some software, and you're going to run version 3.1.2, and you're going to run it with these spec, specified parameters. You have to give all that information because if you don't, if you just say run this software on the raw data file, if the version changes, you might get a totally different answer out. And then you should say things like oh, I ran the software separately for each sample so that people know exactly how you produced the result. And so, if you can't write a computer script, the best case that you can do is be as explicit as possible in this rescue. Go way overboard in the amount of detail you give on how the data got from being the raw data to the processed data.
8:17
So it's pretty important that you do this, and so this is actually I guess a funny example for us, but not such a funny example for Reinhart and Rogoff. These were two authors. And they wrote a paper that talked about growth in the time of debt, and so it was actually the paper that was used to justify austerity in a large number of countries and in politically. And so it turned out that this graduate student got a hold of the Excel file that they used, and he looked at the way that they had processed the data, and he found a large number of errors. And so, paying attention to how the data are collected, and analyzed, and put together, actually caused this paper to be called into serious question, which called, which called into serious question political decisions by a large number of countries in the basically, the recession. So the graduate student in question actually ended up on the Colbert Report, and so he got to actually joke around a little bit about how this Excel error brought down all this political enterprise. But it's an important safety lesson for all of us that we should pay careful attention and keep the script available that takes the raw data and turns it into the processed data that we use for the analysis.
-----

0:00
지난 강의에서 우리는 원시 데이터에 대해, 어디서부터 시작해서, 데이터를 추출하려고 하는 지저분한 파일에 대해 이야기했었습니다. 이 강의에서 우리는 깔끔한 데이터에 대해 이야기 할 겁니다. 이것은 일종의 목표인데, 원시 데이터를 깔끔한 데이터 세트로 변환해서 다운스트림 분석을 할 때, 회귀 모델링 클래스나 예측 최종 기계 학습 클래스에서 배울 수 있다.
0:21
따라서 원시 데이터 세트에서 깔끔한 데이터 세트로 전환한 후에는 다음과 같은 네 가지 사항을 고려해야 한다. 따라서 원시 데이터를 확보해야 합니다, 이 파일이 실제로 정보를 추출하는 파일입니다. 데이터 세트를 깔끔하게 정리하고, 잠시 후에 이야기 할 겁니다. 그리고 각 변수를 설명하는 코드북을 가지고 있어야 합니다, 그리고 그것은 깔끔한 데이터 집합의 가치입니다. 이 코드북은 종종 메타 데이터라고 불린다. 즉, 데이터를 둘러싸고 있는 데이터를 말하고자 하는 것이 무엇인지 설명하는 겁니다. 당신은 당신의 깔끔한 데이터 세트의 각 열을 하나의 변수에 대응시킬 수 있고, 당신은 코드 북에 그러한 변수들의 단위와 같은 것들을 묘사하기를 원할 수 있다.
0:58
가끔 놓치기도 하지만, 이 수업에서 많은 것을 배우게 될 중요한 부분은, 명확하고 정확한 레시피로, 1단계에서 2단계와 3단계로 넘어갈 때 사용하는 겁니다. 즉, 원시 데이터에서 처리된 데이터로 옮기기 위해 취한 정확한 단계를 보고할 필요가 있다. 여러분은 다양한 방법으로 그렇게 할 수 있지만, 이 강의의 목적을 위해 여러분이 데이터를 처리하는 데 사용할 수 있는 스크립트를 그리고 그것들은 여러분이 줄 수 있는 일종의 레시피가 될 겁니다. 그리고 이것이 날것에서 깔끔한 데이터로
1:28
따라서 원시 데이터는 다른 수준의 원시 데이터가 있다는 것을 기억하는 것이 중요하다. 이전 강의에서 언급했듯이, 하지만 여러분이 특정한 데이터 세트를 보기 위한 목적을 위해 원시 데이터는 여러분이 접근했던 데이터의 가장 원시적인 형태 입니다. 예를 들어, 이 파일은 여러분이 무엇을 쟀든 당신이 계약한 회사가 당신에게 보낸 10개의 워크시트가 있는 비정형 엑셀 파일일 수도 있고, 엑셀 파일로 당신에게 건네진 것일 수도 있고, API를 스크랩해서 얻은 복잡한 JSON 데이터일 수도 있다. 혹은 현미경을 통해 보고 그 안에 나타난 세포들을 세어보면서 얻은 숫자들을 건네준 것과 같은 것일 수도 있다.
2:11
원시 데이터가 올바른 형식이고 중요한 형식이며 데이터에 대해 소프트웨어를 실행하지 않은 경우 그래서 다른 누군가가 데이터에 대해 소프트웨어를 실행시킬 수도 있지만, 그것이 여러분에게 전달될 때쯤에는, 실행되어지는 소프트웨어가 전혀 없었다. 그리고 데이터 세트에 있는 어떤 번호도 조작하거나 데이터 세트에서 어떤 데이터도 이동하지 않으셨습니다. 그리고 당신은 어떤 식으로든 데이터에 대한 요약을 하지 않았다. 이것이 데이터가 가장 원시적인 형태라는 것을 아는 방법이다. 그리고 그것은 여러분이 수정하지 않은 원시 데이터 중 하나여야 할 하나의 구성 요소인데, 그것은 공동작업자에게 깔끔한 데이터 세트를 전달해야 하는 것이다.
2:45
반면에, 깔끔한 데이터는 전체 과정의 목표 또는 최종 목표입니다. 즉, 각 변수는 정확히 하나의 열에 있어야 한다는 겁니다. 따라서 열당 하나의 변수가 있어야 하며 각 관측치는 다른 행에 있어야 한다. 즉, 트위팅을 하고 있는 많은 사람들에게 특정 변수를 측정했다면, 많은 사용자들이 게시한 트위트의 수를 측정했다고 가정하면, 열의 트위터 수를 측정한 다음, 각 사용자마다 트위트의 수를 다르게 할 수 있을 겁니다. 모든 종류의 변수를 위한 테이블이 하나 있어야 한다. 예를 들어, 만약 여러분이 트위터와 페이스북 등에서 데이터를 수집한다면, 여러분은 각각 하나의 테이블을 가질 수 있을 겁니다. 테이블이 여러 개 있는 경우 해당 테이블을 일치시키려면 테이블에 해당 테이블을 함께 연결할 수 있는 열이 포함되어 있는지 확인하십시오. 그래서 이것은 종종 ID이고, 우리는 나중에 수업시간에 데이터 세트를 병합하는 것에 대해 이야기 할 것이다.
3:45
과거에 내가 찾은 몇가지 다른 중요한 팁들은 나에게 많은 문제를 해결해주었다. 만약 당신이 각 변수 이름의 파일 맨 위에 행을 포함시킨다면, 때때로 이것은 항상 깔끔한 데이터 세트에 나타나지는 않지만, 그것들이 있다면 훨씬 더 유용하다. 또한 변수 이름이 사람이 읽을 수 있는 것이라면 훨씬 더 유용하다. 예를 들어, AgeDx와 반대로 AgeAtDiagnosis와 같은 칼럼 이름을 사용하면 따라서 좀 더 명시적인 측면에서는 약간의 오류가 있는 것이 좋으며, 그런 다음 일반에서는 데이터를 테이블당 하나의 파일에 저장해야 한다. 따라서 어떤 사람들은 하나의 엑셀 파일 안에 여러 개의 엑셀 스프레드시트에 데이터를 저장하는 것이 습관이기 때문에 각각의 스프레드시트를 다른 파일에 저장하는 것이 더 좋다. 그래서 우리는 수업시간에 그것에 대해 이야기 할 것이다. 다음 구성요소는 종종 빠진 부분인 코드북이다. 그래서 이 깔끔한 데이터 세트를 가지고 있는데, 이것은 매우 멋지고 깔끔하고 깔끔한 데이터 세트입니다. 그리고 넓은 데이터 세트에서 많은 작업을 함으로써 그 데이터를 얻게 되는 겁니다. 결국, 여러 다른 변수들에 대한 정보를 구성하는 데이터 집합이 생기고, 그 파일 맨 위에 있는 변수 이름만 나타나서 무슨 일이 일어났는지 알려줄 겁니다. 하지만 당신은 그 변수에 대해 더 많은 정보를 얻고 싶을 것이다. 한 가지 일반적인 예는, 그래서 이 칼럼의 헤딩은 우리가 이번 분기에 벌어들인 돈의 액수일 수도 있고, 여러분은 정말로 그것이 단위인지, 수천 또는 수백만 정도인지 알고 싶을 겁니다.
5:16
당신은 또한 수행된 요약 선택에 대한 몇 가지 정보를 알고 싶을 것이다. 그래서, 그것은 월수입에 대한 변수일 수도 있다. 그래서 중간 매출인지 평균 매출액인지 알고 싶을 겁니다.
5:29
그리고 당신이 사용한 실험연구 디자인에 대한 정보. 그래서 이런 데이터를 수집하는 방법에 대해 알고 싶으실 겁니다. 그냥 데이터베이스에 있었든, 추출했든, 실험을 수행했든, 임의의 실험이었든, A-B 실험이었든, A-B 실험이었든 말이죠.
5:46
따라서 이 문서의 일반적인 형식은 워드나 텍스트 파일이다. 마크다운 파일 같은데. 여러분이 보셨듯이, 마크다운 파일은 데이터 과학에서 흔히 사용되는 형식이다. 반드시 Study Design(스터디 설계)이라는 섹션이 있어야 하며, 이 섹션에는 데이터 수집 방법에 대한 자세한 설명이 있어야 한다. 따라서 어떤 관측치를 수집할지를 선택하는 방법과 같은 것을 말해야만 한다. 데이터베이스에서 추출한 내용, 제외된 내용 등
6:11
또한 모든 변수와 그 단위를 설명하는 코드 북이라는 섹션이 있어야 한다.
6:16
마지막으로, 당신은 지시 목록이 필요하다. 그래서, 당신이 그 모든 정보를 수집해서 그것을 깔끔한 데이터의 특정한 조건으로 사용 가능하게 했다 하더라도, 당신은 원시 데이터로 돌아가서 다시 처리할 수 있어야 하고, 동일한 깔끔한 데이터 세트를 얻을 수 있어야 한다. 만약 그렇게 되지 않는다면, 데이터 처리 파이프라인에 뭔가 문제가 생긴다면, 그것을 확인하고 고칠 수 있을 겁니다. 이상적으로, 이것은 컴퓨터 스크립트가 될 겁니다. 물론 이 수업의 목적상, R을 위해서라도 파이썬에서도 할 수 있을 겁니다. 스크립트의 입력은 원시 데이터 입니다. 그래서 그 결과물은 처리되고 깔끔한 데이터가 될 겁니다. 그리고 이 스크립트에서 정말 중요한 요소는 매개 변수가 없다는 겁니다. 다시 말하면, 모든 과정을 마친 후에 여러분이 한 모든 것을 고치고 최종 사용자가 수정하거나 수정하지 않아도 되는 정확한 조리법을 가지고 있다는 겁니다. 그리고 그들은 동일한 원시 데이터를 넣으면 동일한 깔끔한 데이터를 얻을 것이다.
7:12
그래서 어떤 경우에는, 모든 단계를 대본으로 쓰는 것이 가능하지 않을 수도 있다. 예를 들어, 데이터에 대해 할 수 있는 모든 것이 아니라, R에서 할 수 있는 겁니다. 내가 그렇게 말하고 있다는 것이 믿기 어렵다는 것을 알지만, 그것은 사실이다. 그래서 여러분이 해야 할 일은 R 스크립트, 그리고 여러분이 사용할 수 있는 다른 프로그래밍 언어들의 다른 스크립트들 외에도, 여러분은 실제로 텍스트 파일로 쓰여진 이런 일련의 명령들이 필요할 수도 있다. 원시 파일을 가지고, 소프트웨어를 실행하고, 버전 3.1.2를 실행하고, 이 특정한 매개 변수를 이용해서 만약 당신이 이 모든 정보를 주지 않는다면, 만약 당신이 원시 데이터 파일에서 이 소프트웨어를 실행한다고 말한다면, 만약 버전이 바뀌면, 당신은 완전히 다른 대답을 얻을 수 있기 때문이다. 그리고 당신은 다음과 같이 말해야 한다. 오, 나는 소프트웨어를 각 샘플마다 따로 실행했다. 그래서 사람들은 당신이 어떻게 결과를 생산했는지 정확히 알 수 있었다. 컴퓨터 스크립트를 작성할 수 없다면, 이 구조에서 가능한 한 데이터가 원시 데이터에서 처리된 데이터로 어떻게 생성되었는지 자세히 설명하는 것은 지나치다.
8:17
이 일을 하는 것은 꽤 중요한 일이지요. 그래서 이것은 사실 우리에게 재미있는 예라고 생각합니다만, 라인하트와 로고프에게는 그렇게 재미있는 예는 아니에요. 이들은 두 명의 작가였다. 그리고 그들은 부채 시기에 성장을 언급하는 논문을 썼고, 그래서 그것은 실제로 많은 나라들과 정치적으로 긴축정책을 정당화하기 위해 사용되었다. 그래서 이 대학원생이 그들이 사용한 엑셀 파일을 손에 넣었고, 그들이 데이터를 처리하는 방법을 보았고, 그는 많은 오류를 발견했다. 그래서 자료가 어떻게 수집되고, 분석되고, 통합되는지에 관심을 기울이면서, 이 논문이 심각한 문제로 대두되었고, 이를테면 기본적으로 많은 국가들이 심각한 정치적 결정을 내린, 불황이라고 부른다. 문제의 대학원생은 실제로 콜버트 보고서(Colbert Report)에 등록하게 되었고, 그래서 그는 실제로 어떻게 엑셀의 실수가 이 모든 정치적 기업을 무너뜨렸는지에 대해 농담을 하게 되었다. 하지만 이것은 우리 모두에게 중요한 안전 교훈이다. 우리는 조심해서 원시 데이터를 취해서 분석에 사용하는 처리된 데이터로 바꾸는 스크립트를 사용할 수 있도록 해야 한다.